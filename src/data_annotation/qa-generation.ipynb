{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic QA pairs for each document.\n",
    "\n",
    "Adapted from Unsloth AI's official inference scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-22T01:23:47.784588Z",
     "iopub.status.busy": "2024-10-22T01:23:47.783859Z",
     "iopub.status.idle": "2024-10-22T01:26:54.725815Z",
     "shell.execute_reply": "2024-10-22T01:26:54.724619Z",
     "shell.execute_reply.started": "2024-10-22T01:23:47.784502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install unsloth\n",
    "!pip install --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip uninstall transformers -y && pip install --upgrade --no-cache-dir \"git+https://github.com/huggingface/transformers.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T02:03:56.321548Z",
     "iopub.status.busy": "2024-10-22T02:03:56.320604Z",
     "iopub.status.idle": "2024-10-22T02:04:11.377077Z",
     "shell.execute_reply": "2024-10-22T02:04:11.376306Z",
     "shell.execute_reply.started": "2024-10-22T02:03:56.321497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T02:04:22.282292Z",
     "iopub.status.busy": "2024-10-22T02:04:22.281870Z",
     "iopub.status.idle": "2024-10-22T02:04:22.293982Z",
     "shell.execute_reply": "2024-10-22T02:04:22.293069Z",
     "shell.execute_reply.started": "2024-10-22T02:04:22.282245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 8192,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T05:16:22.061116Z",
     "iopub.status.busy": "2024-10-22T05:16:22.060460Z",
     "iopub.status.idle": "2024-10-22T05:16:22.069237Z",
     "shell.execute_reply": "2024-10-22T05:16:22.068403Z",
     "shell.execute_reply.started": "2024-10-22T05:16:22.061059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt1 = f\"\"\"\n",
    "You are a helpful AI agent with the task of generating question-answer pairs accurately. Here are examples of factual question-answer pairs based on a given context:\n",
    "\n",
    "Example 1:\n",
    "Context: Pittsburgh, named after William Pitt, 1st Earl of Chatham, is known for its **steelproduction** history. The city is a hub for **education and technology,** with institutions like **Carnegie Mellon University (CMU)** and the University of Pittsburgh. In 1980, it hosted the first **International Conference on Machine Learning (ICML),** marking its role in AI research. $$Techforward?\n",
    "The **PPG Paints Arena** is home to the **Pittsburgh Penguins (NHL)** and hosts events, like **Billie Eilish‚Äôs** concert on **October 13, 2024**. Fans are already booking tickets for the show‚Äîdon‚Äôt miss out! **Check availability: ticketpage_postererror404**. Pittsburgh, with its population of **300k** and over **2.3 million** in the metro area, is growing in innovation and culture. üéüÔ∏è.\n",
    "The city has 446 bridges‚Äî**more than Venice**‚Äîand is called the **‚ÄúCity of Bridges.‚Äù** Take a walk along the rivers (Monongahela, Allegheny, Ohio) to experience it yourself. Error: **riverwalk_mapunavailable.** Pittsburgh‚Äôs transformation includes a booming **tech industry,** with startups in the Strip District and innovation at **CMU.** UPMC is the largest employer. Explore more at **pgh_innovation.html**.\n",
    "For concert tickets or more info on upcoming events at **PPG Arena,** visit [tickets-2024.com/billieeilish]. Not finding answers? Try **404-error/subpage**. See the **AndyWarholMuseum** or **PointStatePark** while you‚Äôre in town. Pittsburgh offers everything from sports to tech‚Äîstay updated with **eventlist@pittsburgh-events**!\n",
    "\n",
    "Q: Who is Pittsburgh named after? A: William Pitt\n",
    "Q: What famous machine learning venue had its first conference in Pittsburgh in 1980? A: ICML\n",
    "Q: What musical artist is performing at PPG Arena on October 13? A: Billie Eilish\n",
    "\n",
    "Now, based on the following context, generate 2-3 important factual question-answer pairs that are highly relevant to the facts in the context. Each question should be clear,important, concise, and directly related to the facts provided. Ensure the answers are concise and accurate. Prioritize the most important or unique details from the context. Think deeply step by step to make sure one of the questions is complex from a long-context dependency standpoint.\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = f\"\"\"\n",
    "You are a helpful AI agent with the task of generating question-answer pairs accurately. Here are examples of factual question-answer pairs based on a given context:\n",
    "\n",
    "Example 1:\n",
    "Pittsburgh, named after William Pitt, 1st Earl of Chatham, is known for its rich industrial history, particularly in steel production. It has transformed into a hub for education and technology, home to prestigious institutions like Carnegie Mellon University (CMU) and the University of Pittsburgh. In 1980, Pittsburgh hosted the first International Conference on Machine Learning (ICML), establishing its connection to AI research.\n",
    "\n",
    "Context: The city also boasts a vibrant cultural scene, highlighted by the PPG Paints Arena, home to the NHL‚Äôs Pittsburgh Penguins. On October 13, 2024, Grammy-winning artist Billie Eilish will perform at the arena, drawing fans from across the region. With a population of over 300,000 and a greater metropolitan area exceeding 2.3 million, Pittsburgh continues to thrive as a center of innovation and culture.\n",
    "Q: Who is Pittsburgh named after? A: William Pitt\n",
    "Q: What famous machine learning venue had its first conference in Pittsburgh in 1980? A: ICML\n",
    "Q: What musical artist is performing at PPG Arena on October 13? A: Billie Eilish\n",
    "\n",
    "Now, based on the following context, generate exactly 3 factual question-answer pairs that are highly relevant to the facts in the context. Each question should be clear,important, concise, and directly related to the facts provided. Ensure the answers are concise and accurate. Prioritize the most important or unique details from the context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T06:20:35.677288Z",
     "iopub.status.busy": "2024-10-22T06:20:35.676875Z",
     "iopub.status.idle": "2024-10-22T06:21:15.380323Z",
     "shell.execute_reply": "2024-10-22T06:21:15.378861Z",
     "shell.execute_reply.started": "2024-10-22T06:20:35.677247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_qa_pairs(qa_text):\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    lines = qa_text.split('\\n')\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"Q:\"):\n",
    "            question = line.replace(\"Q:\", \"\").strip()\n",
    "            questions.append(question)\n",
    "        elif line.startswith(\"A:\"):\n",
    "            answer = line.replace(\"A:\", \"\").strip()\n",
    "            answers.append(answer)\n",
    "\n",
    "    return questions, answers\n",
    "\n",
    "def generate_qa_pairs(input_file, output_file, metadata_file, start_line, end_line):\n",
    "    question_id = 1\n",
    "\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile, open(metadata_file, 'w') as meta_outfile:\n",
    "        for line_num, line in enumerate(tqdm(infile, desc=\"Processing entries\")):\n",
    "            if line_num < start_line:\n",
    "                continue\n",
    "            if line_num >= end_line:\n",
    "                break\n",
    "\n",
    "            entry = json.loads(line)\n",
    "\n",
    "            # Extract metadata\n",
    "            source_id = entry.get('source_id', '')\n",
    "            chunk_id = entry.get('chunk_id', '')\n",
    "            source_name = entry.get('source_name', '')\n",
    "            \n",
    "            text_content = entry.get('text_content', '')\n",
    "\n",
    "            if not text_content:\n",
    "                continue\n",
    "\n",
    "            if isinstance(text_content, list):\n",
    "                text_content = \" \".join(text_content)\n",
    "\n",
    "            # Build prompt for question generation\n",
    "            prompt = prompt1 + f\"\"\"\n",
    "            Context: {text_content}\n",
    "            \n",
    "            Output format:\n",
    "            \n",
    "            Q: <question>, A: <answer>.\"\"\"\n",
    "\n",
    "            messages = [{\"from\": \"human\", \"value\": prompt}]\n",
    "            inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "            returns = model.generate(input_ids=inputs, max_new_tokens=256, use_cache=True)\n",
    "            generated_text = tokenizer.decode(returns[0], skip_special_tokens=True)\n",
    "            sections = generated_text.split(\"<answer>\")\n",
    "\n",
    "            # Take the last part after the \"Context:\", which contains the relevant Q&A pairs\n",
    "            relevant_section = sections[-1].strip()\n",
    "            print(relevant_section)\n",
    "\n",
    "            questions, answers = parse_qa_pairs(relevant_section)\n",
    "\n",
    "            # Add generated questions and answers to the original entry\n",
    "            entry['questions'] = questions\n",
    "            entry['answers'] = answers\n",
    "\n",
    "            # Write the modified entry to the output file\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "            # Now write individual Q&A pairs with metadata to another file\n",
    "            for q, a in zip(questions, answers):\n",
    "                qa_metadata = {\n",
    "                    \"source_id\": source_id,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"source_name\": source_name,\n",
    "                    \"question_id\": question_id,\n",
    "                    \"question\": q,\n",
    "                    \"gt_answer\": a\n",
    "                }\n",
    "                json.dump(qa_metadata, meta_outfile)\n",
    "                meta_outfile.write('\\n')\n",
    "\n",
    "                question_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/path/to/input.jsonl'\n",
    "output_file = '/path/to/output.jsonl'\n",
    "metadata_file = '/path/to/metadata.jsonl'\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "generate_qa_pairs(input_file, output_file, metadata_file, start_line=0, end_line=2500)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9917898,
     "datasetId": 5916702,
     "sourceId": 9686752,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
